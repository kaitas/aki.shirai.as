---
author: aki
title: "\n\t\t\t\tSIGGRAPHってなんですか？(4)「見て信じられる体験と、見て体験しても信じられない技術」\t\t"
slug: vr-soudan-09
id: 30234
date: '2016-09-03 11:44:33'
layout: post
comments: false
categories:
  - VRおもしろ相談室
tags:
  - MoguraVR
  - SIGGRAPH
  - SIGGRAPH2016
  - 白井博士のVRおもしろ相談室
---

本原稿はMoguraVRでの連載記事「[白井博士のVRおもしろ相談室](http://www.moguravr.com/sirai-hakase-siggraph4/)」の再掲です。 **SIGGRAPH****ってなんですか？****(4)****「見て信じられる体験と、見て体験しても信じられない技術」****～白井博士の****VR****おもしろ相談室****第****9****回～** 複数回にわたり、アメリカ・アナハイムで開催されたコンピュータグラフィックスとインタラクティブ技術の国際会議 ACM SIGGRAPH 2016 の情報をお送りしています。 前回、発展途上の先端技術を扱うセッション「**エマージングテクノロジーズ**」（Emerging Technologies；以下[E-Tech](http://s2016.siggraph.org/content/emerging-technologies)）の展示を(1)巨大ロボット, (2)触覚, (3)AR/MR, (4)視覚工学, (5)知覚心理, (6)物理変換の6分類にして、前半を紹介しました。今回はその後半を紹介します。  

## (4)視覚工学

SIGGRAPHはCGとインタラクティブ技術の国際会議ではあるけれど、そのグラフィックスを表示する技術への挑戦もあり、常に新しいディスプレイに提案を見ることができます。ディスプレイ、特に視覚工学分野へのチャレンジとしては日本の「[FOVE](http://www.getfove.com/jp/)」による視線追従型HMD、そしてスタンフォード大学「[Computational Focus-Tunable Near-Eye Display](http://dl.acm.org/citation.cfm?id=2929470)」は、視度補正をメカ制御で行うHMDを提案していました。

### NVIDIAによる中心窩レンダリング

その中でもNVIDIAは[SMI社の超高速視線追従型HMD](http://www.moguravr.com/sensomotoric/)と連携し、中心窩（ちゅうしんか; fovea centralis）の高解像度性を利用したレンダリング技術「[Perceptually Based Foveated Virtual Reality](https://research.nvidia.com/publication/perceptually-based-foveated-virtual-reality)」を展示し高く評価されました。 https://www.youtube.com/watch?v=BjV0WtnjmmE 中心窩は周辺視野と異なり、高解像度です。このデモでは、注視点追跡機能を統合したレンダリング手法を開発し、Oculus DK2をベースとしたSMI社によるプロトタイプのHMDに実装し、周辺視野のみに低品質のグラフィックスを表示することで、見た目の劣化なく、計算コストの大幅な削減を実現しています。コントローラのトリガーを使い、自分の視線追従を固定することでその効果を確認できるのですが、解像度だけでなく、コントラストにおいても周辺視野では知覚上の色差を感じることが難しいことを体験できました。通常の数倍のレンダリングコストの節約、さらにポストエフェクトの処理などの応用を考えても、自己視点のVRならではの手法として今後の幅広い活用が期待できそうです。  

## (5)知覚心理

[知覚心理学](https://ja.wikipedia.org/wiki/%E7%9F%A5%E8%A6%9A%E5%BF%83%E7%90%86%E5%AD%A6)は人間の知覚のあり方を研究する一分野です。すでにE-Techでの視覚や触覚については紹介してきましたが、あえて「VRを用いたイリュージョン」として、不思議な感覚を味わえる2つの展示を紹介します。

### FlyVIZ: パノラマ画像のまま歩く

フランス西部の大学連携研究所 ESIEA - INSA - Inriaによる「[Enjoy 360° vision with the FlyVIZ](http://dl.acm.org/citation.cfm?id=2929471)」はGoPano+iPhone4S+Oculus DK1による、360度パノラマ映像のまま歩行可能にする超広視野視覚装置です。 https://www.youtube.com/watch?v=g0w4D05Fdho ちゃんと歩けるのが面白い。ハイスペックPCを一切使っていないにもかかわらず、ディレイ少なく処理できる装置構成にも注目です。

### Unlimited Corridor - VRで無限回廊を実現

東京大学 廣瀬研究室の「[Unlimited Corridor: Redirected Walking Techniques Using Visuo-Haptic Interaction](http://dl.acm.org/citation.cfm?id=2929482)」はすでに[MoguraVRでも紹介](http://www.moguravr.com/unlimited-corridor-vr-repo/)されていますが、巨大な８の字状の回廊をHMD装着で直線的に歩行したと知覚させることで、無限の歩行空間を実現する知覚心理応用技術です。 http://www.youtube.com/watch?v=DFT4cjBLuLU 過去の研究において、VRによるリダイレクト歩行(redirected walking; RDW, 歩行の物理的な置き換え)、具体的には直線的に対して矛盾を検出させずに円弧で歩かせる場合には、少なくとも22メートルの半径を有する円弧にリダイレクトさせることが必要という[研究](http://www.ncbi.nlm.nih.gov/pubmed/26357104)がありましたが、この技術はマルチモダリティ、つまり「壁を触る」という視覚以外の知覚によって、より狭い空間で実現しています。ユーザがまっすぐ無限大の通路を歩き、また自由に分岐することができる動的歪み量変更可能なアルゴリズムを開発しており、複数のユーザーが同時に無限回廊を歩くことができます。

### 魔法のような歩行誘導

筑波大学 落合研究室の「Graphical Manipulation of Human’s Walking Direction with Visual Illusion（視覚的錯覚を用いた歩行方向の映像誘導）」は、HMD装着の歩行者を無意識に制御可能にする新しい手法です。「右へ曲がれ」のような意識下の指示情報を提示せず、設計者が意図した方向に体験者を歩かせることができます。 https://www.youtube.com/watch?v=qnYCbd41XvY ビデオシースルーHMD越しに「A」「B」2つの目標が表示されており、体験者は「Bに向かうように指示される」のですが、結果的には「Aに必ずたどりつく」体験ができます。システムはカメラ映像に対し画像処理を行っており、ユーザの視界に対して視覚的な補正を与えています。仕掛けは簡単ですが、まるで魔法で操られているようです。

## (6)物理変換

最後に「物理的な変換」もしくは「物理的に変換」を「物理変換」としてまとめてみました。前回紹介した巨大ロボット「[Big Robot Mk.1A](https://www.youtube.com/watch?v=ugP02aJHDUA)」も、物理的な変換に混ぜられるかもしれませんね。

### 断面画像を空中像で表現

https://www.youtube.com/watch?v=HsaD7qsN1ZA 東京大学 篠田・牧野研究室の「[X-SectionScope: Cross-Section Projection in Light-Field Clone Image](http://dl.acm.org/citation.cfm?id=2929483)」は、空中に断面画像を重畳するリアルタイム3Dディスプレイを提案しました。空中像ディスプレイにより、X線画像のような物体の内部映像を、見ることができます。 一見、単純なデモではありますが、実は撮像系が興味深く、ライトフィールドクローン（LFC）の画像を再現するために、2つのマイクロミラーアレイプレート（MMAPs）を使用しています。空中像ディスプレイはアスカネット社の[エアリアルイメージ](http://aerialimaging.tv/)を使用しているようです。

### 光線空間ディスプレイを照明に使う

ソニーコンピュータサイエンス研究所による「AnyLight： An Integral Illumination Device」は、光線の方向を制御する3D表示技術である裸眼立体ディスプレイを、動的照明に応用したものといえます。 https://www.youtube.com/watch?v=WDn63RwKlPc プロジェクターを使ってライトフィールドディスプレイを構成しています。フライアイレンズ、つまり昆虫の複眼のような形状のレンズは光硬化樹脂で作っているそうです。余計な光を通さないように、マスクをしています。動画では色を持った面光源のライトの方向と影の方向が変化していることがわかります。

### LightAir: Droneを三脚にする

ロシアのスコルコボ工科大学の「LightAir: a Novel System for Tangible Communication With Quadcopters Using Foot Gestures and Projected Images」は、プロジェクターと画像認識のためのカメラをクワッドコプターに搭載したインタラクション技術を発表しました。 http://www.youtube.com/watch?v=n9h7zW1qhkQ 音と風圧は問題かもしれませんが、サッカー場などでは使えるかもしれない。

### ディープラーニングで椅子が自在に動く

http://www.youtube.com/watch?v=NgaCq1sD2Nc

元東京大学 石川研究室、現・韓国KAISTに在籍するAlvaro Cassinelliらによる「[Ratchair: Furniture That Learns to Move Itself With Vibration](http://mid.kaist.ac.kr/projects/ratchair/)」はバイブレーターのみで椅子を自在に動かします。ディープラーニングで任意の方向に進むように学習させた結果と、スマートフォンを使って本当に前後左右に動きます（時間はかかりますが）。

### 浄瑠璃を一人で演じる

筑波大学 落合研究室は学部生を中心とした展示で「Yadori: Mask-Type User Interface for Manipulation of Puppets」も展示しており、人形浄瑠璃をKinectとHMDと一体化した口形状センシングと統合して実現し、パペットの視点から見た映像と、リアルタイムの演技を融合するデモも展示していました。 https://www.youtube.com/watch?v=7l6fDFHp9zo ちゃんと自分の視点が見えるのはすごいと思いましたが、カエルのパペットが喋っている時の視点ってものすごく揺れるので、酔うわけではないのですが、慣れは必要ですね。ゆるキャラ着ぐるみの操作には使えそう。

### 実物体回転による物性表現

東京大学 石川・渡辺研究室は例年、高速コンピュータビジョンに関わる研究を発表していましたが、今年は2件の全く**毛色の違う**、興味深い発表を行っていました。 「[ZoeMatrope: A System for Physical Material Design](http://www.k2.t.u-tokyo.ac.jp/vision/ZoeMatrope/index-j.html)」は、自由な表面材質を作り出せるディスプレイです。<span style="line-height: 1.625;">あらかじめ複数の素材となる表面材質の異なる球体を高速ターンテーブルに準備し、高速プロジェクタ[DynaFlash](http://www.k2.t.u-tokyo.ac.jp/vision/dynaflash/index-j.html) (石川研究室による1,000fps・3ms遅延で8bit階調の映像を投影する[高速プロジェクタ](https://www.youtube.com/watch?v=L8kjdObjZpY))の制御で任意の色や表面材質（輝き、粗さ、拡散反射）を表現します。事前に物質を選ぶためのアルゴリズムと、GUIによる任意変更が研究のポイントと思います。</span> https://www.youtube.com/watch?v=w0ZPvTYzbsc もう一つのデモ、「[Phyxel: Realistic Display Using Physical Objects With High-speed Spatially Pixelated Lighting](http://www.k2.t.u-tokyo.ac.jp/vision/Phyxel/index-j.html)」もターンテーブルを使用していますが、こちらは、毛糸や木材などより具体的な材料を用意し、高速プロジェクタによって任意の文字や形状などを表示できる、より表現力の高いデモ。双方ともシンプルなアイディアですが、大変強力な結果であり、応用の可能性も高そう。 https://www.youtube.com/watch?v=x-QC7g_CI1k 以上でほぼすべてのE-Tech作品を紹介し終わりました！

## 優秀デモ作品への表彰

今年のE-Techでは新たな試みとして、公式のAward表彰を行いました。まず「**Best E-Tech Award**」として、スタンフォード大の顔すげ替え技術「Demo of Face2Face」を選定しました。ACM SIGGRAPHと提携関係にあるフランス・Laval Virtualは「**Laval Virtual Award**」としてロシアの「LightAir」とNVIDIAの「Perceptually Based Foveated Virtual Reality」を選出しました来年3月22〜26日に開催されるLaval Virtual ReVolution 2017「[TransHumanism++](http://www.laval-virtual.org/en/prices-competitions/revolution/introduction-revolution.html)」にて招聘展示の予定です。日本からはデジタルコンテンツ協会から「**Digital Contents EXPO Award**」が韓国KAIST「Ratchair」に送られました。表彰式の様子は[Periscopeで録画しておきました](https://www.periscope.tv/o_ob/1nAJEbkjYnbxL)のでご参照ください。

## 今年のE-Techにはなぜ日本人の展示が多いのか？

SIGGRAPH 2016は E-Tech チェアが日本のVR系研究者・稲見昌彦先生（東大）だったこともあり、協賛企業にNTT、NTTコミュニケーションズ、ドワンゴ、スマートニュースの日本企業が参加しており、日本からの投稿も多かったようです。採択された24件のうち、日本からの出展が3/4を占めていました。ただし、この種の国際審査ではよくあるコンフリクト回避（関係機関や利害関係がある場合は審査に関わらず退席する）を「真面目に行なった結果、こうなってしまった」（稲見先生談）ということで、国の属性が入っているわけではなく、世界から見た「日本の研究者の層の厚さと実力」と考えて良いと思います。 実際には日本からの投稿も大変多く、採択されるのは至難の技。イベントや共同研究などで日本の研究に触れやすい距離にいる日本のVR開発者は"恵まれている"と言えるかもしれませんね。

## まとめ

### 見て信じられる体験と、見て体験しても信じられない技術

  エマージングテクノロジーズ、つまり「発展途上の技術」というセッション名からもわかるように<span style="font-weight: 400;">、まだ野とも山ともつかない技術です。見て信じられる技術もありますが、見ても体験しても信じられないような技術もあります。これらを</span>直接見て、触れて、体験できるという価値は大変大きいです。しかも研究者に直接会うこともできます。 魔法のような体験や触覚のような知覚心理系の技術は、YouTube等の動画で見るよりも実際に見て触ってみるしかありません。会ってみることで、さらに使える技術なのか、ラボ内限定技術なのかがわかります。 日本国内では9月中旬に開催される[日本VR学会大会](http://www.vrsj.org/events/annual_conference/)と10月に開催される[デジタルコンテンツエキスポ](https://www.dcexpo.jp/)が最も良い機会ですが、それらの展示を生み出している研究者たちの年間スケジュールの中で最も大事な国際会議の一つが、SIGGRAPHです。一つの目標であり、世界への架け橋でもあります。 今回のSIGGRAPHでは、科学技術よりの技術デモをE-Tech、もう一つはコンテンツよりの作品や技術展示を集める VR Village がありました。この2つは従来はそれぞれ別の審査方式を採っており、別の投稿として提案する必要がありましたが、今回のSIGGRAPHでは「General Submission」としていったん一つの投稿として論文アブストラクト、ビデオ、展示フロアプランなどを提出した上で、さらに審査員側で「E-Tech向きか？VR Village向きか？」が検討されたようです。今回のVR Villageは従来のデモ展示に加え、HMDコンテンツ中心の **StoryLab** と プレゼンテーションの3部門が用意されていました。 次回はこれらのコンテンツに近い展示について、紹介したいと思います。 [![SIGGRAPH2016](https://aki.shirai.as/wp-content/uploads/2016/08/SIGGRAPH2016-1024x768.jpg)](https://aki.shirai.as/wp-content/uploads/2016/08/SIGGRAPH2016.jpg) **—** **白井博士（しらいはかせ）** 1996年 東京工芸大学工学部写真工学科卒、1998年 東京工芸大学大学院工学研究科画像工学専攻修士課程修了。キヤノングループが開発した産業用ゲームエンジン「RenderWare」の日本事務所立ち上げを経て、2001年 東京工業大学大学院総合理工学研究科知能システム科学専攻博士後期課程に復学。2003年博士 (工学)の学位を取得。2003～2004年に財団法人NHKエンジニアリングサービス・次世代コンテント研究室、2004年末にフランスに渡り、国立工芸大学（ENSAM/ParisTech）客員研究員。VRによる地域振興、国際VR作品公募展Laval Virtual ReVolutionを2006年より主催。2007年より帰国し、日本科学未来館科学コミュニケーターを経て、現在、神奈川県の大学でVRエンタテイメントシステムの開発者を育成しながらVR作家として活動。 <著書等> 「白井博士の未来のゲームデザイン -エンターテインメントシステムの科学」(単著)、「WiiRemoteプログラミング」(共著)，日本科学未来館企画展 GameOn公式図録「ゲームってなんでおもしろい？」(インタビュー)，「ゲームクリエイターが知るべき97のこと 2」（執筆協力）など。 blog: [http://aki.shirai.as/](http://aki.shirai.as/) Twitter: [@o_ob](https://twitter.com/o_ob)